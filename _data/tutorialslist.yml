- title: How to Execute Gradients Generated by Clad on a CUDA GPU
  author: Ioana Ifrim
  abstract: |
    CLAD provides automatic differentiation (AD) for C/C++ and works without
    code modification (legacy code). Given that the range of AD application
    problems are defined by their high computational requirements, it means
    that they can greatly benefit from parallel implementations on graphics
    processing units (GPUs).
    This tutorial showcases how to firstly use CLAD to obtain your function’s
    gradient and how to schedule the function’s execution on the GPU.

  url: /tutorials/clad_cuda_simple/
  date: 2021-08-20

- title: Interactive Automatic Differentiation With Clad and Jupyter Notebooks
  author: Ioana Ifrim
  abstract: |
    xeus-cling provides a Jupyter kernel for C++ with the help of the C++
    interpreter cling and the native implementation of the Jupyter protocol
    xeus. Within the xeus-cling framework, CLAD can enable automatic
    differentiation (AD) such that users can automatically generate C++ code
    for their computation of derivatives of their functions. In mathematical
    optimization, the Rosenbrock function is a non-convex function used as a
    performance test problem for optimization problems, this tutorial shows the
    computation of the function’s derivatives, by employing either CLAD’s
    Forward Mode or Reverse Mode.

  url: /tutorials/clad_jupyter/
  date: 2021-08-20
